# -*- coding: utf-8 -*-
"""Thai_Sentiment_Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iyF_d91W1jrf1upEwqZcx8oH6m6Epdxp

# ข้อมูลตัวอย่าง
"""

text_list = ['''เฟ็ดเฟ่อย่างกับเดอะค็อป 999 (คลิป) <br> #ข่าวสด #เด่นออนไลน์ <br>รีบดูก่อนโดนลบ! เฟ็ดเฟ่สร้างความฮึกเฮิม นั่งหลังคารถชูถ้วยจุดพลุ มั่นใจมาแน่นอน 
            <br>บิวด์กันเข้าไป ก่อนเกมการแข่งขัน ยูฟ่าแชมเปียนส์ลีก รอบ … <br><img src='https://xxx'>''',
             
            '''แอดขอให้คุณผู้อ่านใช้วิจารณญาณและดุลยพินิจในการอ่าน คำพูดที่สอบถามคุณยายเป็นเพียงความข้างเดียวที่ออกมาจากใจและปากของคุณยาย ยังไม่มีโอกาสไปสอบถามความ
            อีกมุมจากตัวลูกชายคนเล็ก #แม่เฒ่าถูกไล่ #แม่เฒ่าขาพิการ #พาหลานชายพิการนอนข้างทาง <br>'อย่าว่าเขาๆคงมีเหตุผล' 'ยาย'ถูก'ลูก'ไล่-ขอร้องแม้ปวดใจ <br>
            ประชาชนที่รู้ข่าวต่างนำอาหารกับน้ำดื่ม ไปให้ 2 ยายหลานผู้พิการได้ดื่มกิน หลังถูกลูกชายไล่จนต้องนอนศาลาร.... <br><img src='https://xxx'>''']

"""**Step 1: Text cleaning**"""

import re
import string

def clean_msg(msg):
    
    
    # ลบ text ที่อยู่ในวงเล็บ <> ทั้งหมด
    msg = re.sub(r'<.*?>','', msg)
    
    # ลบ hashtag
    msg = re.sub(r'#','',msg)
    
    # ลบ เครื่องหมายคำพูด (punctuation)
    for c in string.punctuation:
        msg = re.sub(r'\{}'.format(c),'',msg)
    
    # ลบ separator เช่น \n \t
    msg = ' '.join(msg.split())
    
    return msg

print('original text:\n',text_list[0])
print('clean text:\n',clean_msg(text_list[0]))

print('original text:\n',text_list[1])
print('clean text:\n',clean_msg(text_list[1]))

clean_text = [clean_msg(txt) for txt in text_list]
clean_text

"""**Step 2: Tokenize**"""

!pip install pythainlp
!pip install stop_words
import pythainlp
from pythainlp import word_tokenize
from pythainlp.corpus import thai_stopwords
from pythainlp.corpus import wordnet
from nltk.stem.porter import PorterStemmer
from nltk.corpus import words
from stop_words import get_stop_words

import nltk
nltk.download('words')
th_stop = tuple(thai_stopwords())
en_stop = tuple(get_stop_words('en'))
p_stemmer = PorterStemmer()

def split_word(text):
            
    
    tokens = word_tokenize(text,engine='newmm')
    
    # Remove stop words ภาษาไทย และภาษาอังกฤษ
    tokens = [i for i in tokens if not i in th_stop and not i in en_stop]
    
    # หารากศัพท์ภาษาไทย และภาษาอังกฤษ
    # English
    tokens = [p_stemmer.stem(i) for i in tokens]
    
    # Thai
    tokens_temp=[]
    for i in tokens:
        w_syn = wordnet.synsets(i)
        if (len(w_syn)>0) and (len(w_syn[0].lemma_names('tha'))>0):
            tokens_temp.append(w_syn[0].lemma_names('tha')[0])
        else:
            tokens_temp.append(i)
    
    tokens = tokens_temp
    
    # ลบตัวเลข
    tokens = [i for i in tokens if not i.isnumeric()]
    
    # ลบช่องว่าง
    tokens = [i for i in tokens if not ' ' in i]

    return tokens

print('tokenized text:\n',split_word(clean_msg(text_list[0])))
print('tokenized text:\n',split_word(clean_msg(text_list[1])))

tokens_list = [split_word(txt) for txt in clean_text]

tokens_list

"""**Step 3 + 4a: Bag of words + count word**

count vectorizer คือการนำกลุ่มของtoken มีสร้างเป็นmatrix โดยใช้กลุ่มของคำที่มีเป็นตัวอ้างอิง คำที่มีในประโยคจะถูกตั้งค่าเป็น 1 คำที่ไม่มีจะเป็น 0 เช่น มีกลุ่มของคำ [“This”, “is”, “am”, “are”, “a”, “be”, “test”, “word”, “sentence”] ประโยค “This is a test sentence” จะแปลงเป็น matrix ได้ดังนี้ [1, 1, 0, 0, 1, 0, 1, 0 ,1]
"""

from sklearn.feature_extraction.text import CountVectorizer
tokens_list_j = [','.join(tkn) for tkn in tokens_list]
cvec = CountVectorizer(analyzer=lambda x:x.split(','))
c_feat = cvec.fit_transform(tokens_list_j)

cvec.vocabulary_

c_feat[:,:30].todense()

"""**Step 3 + 4b: Bag of words + tf-idf**

> ดูคำอธิบายเพิ่มเติมที่ https://ichi.pro/th/khwam-ru-beuxng-tn-keiyw-kab-bag-of-words-ni-nlp-189762346660904


"""

from sklearn.feature_extraction.text import TfidfVectorizer
tvec = TfidfVectorizer(analyzer=lambda x:x.split(','),)
t_feat = tvec.fit_transform(tokens_list_j)

t_feat[:,:5].todense()  #tf-idf

print(len(tvec.idf_),len(tvec.vocabulary_))

c_feat[:,:5].todense()  #countvocab

"""# การทำ sentiment analysis ภาษาไทย

บทความโดย อ.ดร.กานต์ ยงศิริวิทย์
วิทยาลัยนวัตกรรมดิจิทัลเทคโนโลยี มหาวิทยาลัยรังสิต
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

"""review_shopping.csv เป็นไฟล์กำกับข้อความรีวิวจากเว็บขายของออนไลน์แห่งหนึ่ง เมื่อปี 2561 โดยมี 2 tag คือ pos และ neg กำกับโดย นาย วรรณพงษ์ ภัททิยไพบูลย์"""

import io
from google.colab import files
uploaded = files.upload()
dataset = pd.read_csv(io.BytesIO(uploaded['review_shopping.csv']))

df = pd.read_csv('review_shopping.csv', sep='\t', names=['text', 'sentiment'], header=None)
df

df['sentiment'].value_counts().plot.bar()  #ดูว่ามีข้อความที่เป็นแง่บวกและแง่ลบ อย่างละกี่ข้อความ

from pythainlp.corpus.common import thai_stopwords
thai_stopwords = list(thai_stopwords())
thai_stopwords

from pythainlp import word_tokenize
def text_process(text):
    final = "".join(u for u in text if u not in ("?", ".", ";", ":", "!", '"', "ๆ", "ฯ"))
    final = word_tokenize(final)
    final = " ".join(word for word in final)
    final = " ".join(word for word in final.split() 
                     if word.lower not in thai_stopwords)
    return final
df['text_tokens'] = df['text'].apply(text_process)
df

from sklearn.model_selection import train_test_split
X = df[['text_tokens']] #feature
y = df['sentiment'] #เฉลย
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)

"""Word Vectorizer และ Bag-of-Words (BoW)
การจะสร้างแบบจำลองวิเคราะห์ความรู้สึกจากข้อความ จำเป็นจะต้องแปลงข้อมูลให้อยู่ในรูปแบบที่มีจำนวนของตัวแปรที่ทำกัน โดยเริ่มต้นเราจะใช้ Count Vectorizer ของ sklearn มาช่วยในการจัดการดึงคำทั้งหมดออกมาจากข้อความ และจัดเก็บในรูปแบบ Vector (จะคล้าย ๆ กับลักษณะของพจนานุกรมที่มีการระบุตัวเลข index ของแต่ละคำด้วย)
"""

from sklearn.feature_extraction.text import CountVectorizer
cvec = CountVectorizer(analyzer=lambda x:x.split(' '))
cvec.fit_transform(X_train['text_tokens'])
cvec.vocabulary_

train_bow = cvec.transform(X_train['text_tokens'])
pd.DataFrame(train_bow.toarray(), columns=cvec.get_feature_names(), index=X_train['text_tokens'])

"""สร้างแบบจำลอง **Logistic Regression** เพื่อจำแนกความรู้สึก positive หรือ negative
ทำการสร้างแบบจำลอง Logistic Regression เพื่อจำแนก (classify) ข้อความว่าเป็น pos หรือ neg โดยให้ตัวแปรต้นเป็น BoW ที่สร้างจากข้อความสำหรับการฝึกฝน และตัวแปรตามคือ y_train หรือก็คือคอลัมน์ sentiment ที่แบ่งไว้สำหรับการฝึกฝน

ดู algorithm อื่น  ๆ ได้ที่ https://scikit-learn.org/stable/
"""

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(train_bow, y_train)

"""ทดสอบแบบจำลอง
ใช้ sklearn ในการทดสอบแบบจำลองว่ามีความแม่นยำมากน้อยแค่ใหน โดยดูจาก precision และ recall ได้เลย ซึ่งโดยแล้วอยู่ที่ประมาณ 0.9 ซึ่งเป็นที่น่าพอใจสำหรับผมนะครับ 555
"""

from sklearn.metrics import confusion_matrix,classification_report
test_bow = cvec.transform(X_test['text_tokens'])
test_predictions = lr.predict(test_bow)
print(classification_report(test_predictions, y_test))

"""ทดสอบกับข้อความที่เราสร้างขึ้นเอง
สามารถทดสอบกับข้อความที่เราสร้างขึ้นเองได้ โดยตัวอย่างตามด้านล่างที่เป็นข้อความว่า ตรงปกส่งไวครับ ผลลัพธ์ที่ได้คือ pos ซึ่งหมายถึงเป็นข้อความในแง่บวก
"""

my_text = 'ตรงปกส่งไวครับ'
my_tokens = text_process(my_text)
my_bow = cvec.transform(pd.Series([my_tokens]))
my_predictions = lr.predict(my_bow)
my_predictions

my_text = 'ไม่ตรงปกส่งช้าครับ'
my_tokens = text_process(my_text)
my_bow = cvec.transform(pd.Series([my_tokens]))
my_predictions = lr.predict(my_bow)
my_predictions

my_text = 'ของที่สั่งไม่มีคุณภาพเลยครับ'
my_tokens = text_process(my_text)
my_bow = cvec.transform(pd.Series([my_tokens]))
my_predictions = lr.predict(my_bow)
my_predictions