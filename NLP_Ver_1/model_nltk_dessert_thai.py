# -*- coding: utf-8 -*-
"""Model_NLTK_Dessert_Thai.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VXCnrZtt6HzjP1VD7BZW0JSzDuGK_n8-
"""

!pip install tensorflow
!pip install keras
!pip install h5py pyyaml

from google.colab import drive
drive.mount('/content/gdrive', force_remount=False)

import os
os.chdir("/content/gdrive/")
os.getcwd()

drive.mount('/content/drive')

import pandas as pd
path = "/content/drive/MyDrive/Colab Notebooks/AI/img_dessert_thai"
files = os.listdir(path)
categories = []

for filename in files:
    category = filename.split('.')[0]
    if 'a' in category:
        categories.append("0")
    elif 'b' in category:
        categories.append("1")
    elif 'c' in category:
        categories.append("2")
    elif 'd' in category:
        categories.append("3")
    elif 'e' in category:
        categories.append("4")
    elif 'f' in category:
        categories.append("5")
    elif 'g' in category:
        categories.append("6")
    elif 'h' in category:
        categories.append("7")
    else: pass
df = pd.DataFrame({
    'filename': files,
    'category': categories
})

print(df)

import numpy as np
from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array

path = "/content/drive/MyDrive/Colab Notebooks/AI/img_dessert_thai"
files = os.listdir(path)

X =  np.empty([0])
y = np.empty([0])
for filename in files:
    print(path+"/"+filename)
    # load the image
    img = load_img(path+"/"+filename)

    print(img.size)
    img.show()

    # convert to numpy array
    img_array = img_to_array(img)

    X = np.append (X,img_array)
    category = filename.split('.')[0]
    if 'a' in category:
        y = np.append(y,[0])
    elif 'b' in category:
        y = np.append(y,[1])
    elif 'c' in category:
        y = np.append(y,[2])
    elif 'd' in category:
        y = np.append(y,[3])
    elif 'e' in category:
        y = np.append(y,[4])
    elif 'f' in category:
        y = np.append(y,[5])
    elif 'g' in category:
        y = np.append(y,[6])
    elif 'h' in category:
        y = np.append(y,[7])
    else: pass

from sklearn.model_selection import train_test_split
X = X.reshape(len(y),100,100,3)
x_vector = 100*100*3
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1000)

#เปลี่ยนมิติข้อมูลให้เป็น 4 มิติ คือ (จำนวนรายการ, Column, Row, จำนวน Layer สี) โดยข้อมูล 100 x 100 ดังนั้นมิติคือ (จำนวนรายการ, 28, 28, 3)
X_train = X_train.reshape(X_train.shape[0], 100, 100, 3)
X_test = X_test.reshape(X_test.shape[0], 100, 100, 3)

#กำหนดตัวแปรมิติข้อมูล เพื่อเตรียมใช้ใน Argument input_shape ใน Layer แรกของ Model โดยไม่ต้องระบุจำนวนรายการ 
#ดังนั้น มิติข้อมูลสำหรับ Layer แรก คือ (100, 100, 3) อนึ่งเราต้องกำหนดมิติข้อมูลสำหรับ Layer แรกเท่านั้น ส่วน Layer อื่น TensorFlow จะอนุมานให้เอง
input_shape = (100, 100, 3)

#แปลง Data type ของข้อมูลให้เป็น Float เพื่อให้ Algorithm สามารถคำนวนและแสดงผลเป็นค่าทศนิยมได้
X_train = X_train[:, :, :, :].astype('float32')
X_test = X_test[:, :, :, :].astype('float32')
y_train = y_train[:]
y_test = y_test[:]

#Scale ข้อมูลโดยใช้วิธี Normalise ซึ่งก็คือการหารข้อมูลทุกรายการด้วย Range ของค่าความสว่างของแต่ละ Pixel นั่นก็คือ 255
X_train /= 255
X_test /= 255

print('X_train shape:', X_train.shape)
print('X_test shape:', X_test.shape)
print('y_train shape:', y_train.shape)
print('y_test shape:', y_test.shape)

import matplotlib.pyplot as plt
label_thai = ['ทองหยอด', 'ขนมหม้อแกง', 'ขนมต้ม', 'ตะโก้', 'ทองม้วน', 'นางเล็ต', 'ข้าวเหนียวมะม่วง', 'เปียกปูน']
label_eng = ['TongYod', 'MorGang', 'KanomTom', 'TaKo', 'TongMoun', 'NangLet', 'MaMoung', 'PiagPoon']
plt.figure(figsize=(10,10))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(X_train[i], cmap=plt.cm.binary)
    plt.xlabel(y_train[i])
plt.show()

from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras.utils import np_utils 
from keras.preprocessing.image import ImageDataGenerator
from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, Flatten
from keras.layers import BatchNormalization
import tensorflow as tf

model = Sequential()

#*BatchNorm คือ เทคนิคที่ใช้ระหว่างการเทรน Machine Learning เพื่อปรับ Shift, Scale ให้ Activation ที่อยู่ภายใน Hidden Layer ของ Deep Neural Network ให้มีขนาดเหมาะสม ไม่เล็ก ไม่ใหญ่เกินไป โดยดูเทียบจาก Mean และ 
#Standard Deviation ของทุก Activation ใน Layer ของทั้ง Batch นั้น คล้ายกับ Feature Scaling ของ Input และมีการเสริมด้วย Learning Parameter เพื่อให้โมเดลเรียนรู้ ที่จะปรับ Activation ให้เป็นที่ต้องการได้เอง

#*Drop out เป็น เทคนิคหนึ่งที่ช่วยแก้ปัญหาไม่ให้ model นั้นเกิดปัญหา Overfitting เกินไป
#*Overfitting พูดง่ายๆ คือ model จำข้อสอบแทนที่จะรู้จักการทำข้อสอบ เวลาไปเจอข้อสอบที่ไม่เคยทำก็ ไปไม่เป็น ประมาณนี้ละกัน

#*padding='same' ปรับภาพให้เป็นต้นฉบับ
#*input_shape ใส่ขนาดรูปภาพและมิติ
#*MaxPooling2D เอาค่าที่มากที่สุดใน pixel นั้น ๆ คือการลดขนาดของรูปภาพ ส่งผลให้จำนวนของ Parameter ที่ใช้ในการประมวลผลลดลง ทำให้การประมวลผลมีความรวดเร็วมากยิ่งขึ้น
#*Flatten() คือปรับให้เป็น 1 มิติ
#*Convolution layer จะมี Kernel ที่ใช้ในการสกัด Feature จากรูปภาพ
#*relu is 0-inf และ activation คือ ว่าจะใช้ fuction ไหนในการ feature leaning
#*Dense คือ node ที่เชื่อมต่อกันทุกตัวเพื่อสกัด feature 
#*softmax ใช้ในตอนที่มี หลายคำตอบ
#*kernel_initializer คือการกระจายค่าน้ำหนักให้มีค่าเท่า ๆ กัน ใน layer
#*he_uniform แนะนำให้นิยมใช้คู่กับ Relu 

model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(100, 100, 3)))
model.add(BatchNormalization())
model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D((2, 2)))
model.add(Dropout(0.2))

model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
model.add(BatchNormalization())
model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D((2, 2)))
model.add(Dropout(0.3))

model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
model.add(BatchNormalization())
model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D((2, 2)))
model.add(Dropout(0.4))

model.add(Flatten())
model.add(Dense(128, activation='relu', kernel_initializer='he_uniform')) #Fully conneccted
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(8, activation='softmax'))

model.summary()

model.compile(optimizer='adam',  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  metrics=['accuracy'])

history = model.fit(X_train, y_train, batch_size=35, epochs=35,validation_data = (X_test,y_test),callbacks =None)

score = model.evaluate(X_train, y_train, verbose = 0)
print('Train loss : ', score[0])
print('Train accuracy : ', score[1])
print("")
score = model.evaluate(X_test, y_test, verbose = 0)
print('Test loss : ', score[0])
print('Test accuracy : ', score[1])

plt.rcParams['figure.figsize'] = (6,6)
plt.plot(history.history['accuracy']) #train
plt.plot(history.history['val_accuracy']) #test
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc = 'upper left')
print(history.history.keys())

plt.show()

model.predict(X_test[2].reshape(1,100,100,3)).argmax() #argmax() ตำแหน่งที่ได้ค่าสูงสุด

import os
import tensorflow as tf
from tensorflow import keras
tf.__version__

def create_model():
  model = tf.keras.model = Sequential([
    keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(100, 100, 3)),
    keras.layers.BatchNormalization(),
    keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),
    keras.layers.BatchNormalization(),
    keras.layers.MaxPooling2D((2, 2)),
    keras.layers.Dropout(0.2),
    keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),
    keras.layers.BatchNormalization(),
    keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),
    keras.layers.BatchNormalization(),
    keras.layers.MaxPooling2D((2, 2)),
    keras.layers.Dropout(0.3),
    keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),
    keras.layers.BatchNormalization(),
    keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),
    keras.layers.BatchNormalization(),
    keras.layers.MaxPooling2D((2, 2)),
    keras.layers.Dropout(0.4),
    keras.layers.Flatten(),
    keras.layers.Dense(128, activation='relu', kernel_initializer='he_uniform'),
    keras.layers.BatchNormalization(),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(8, activation='softmax')
  ])
  model.compile(optimizer='adam',  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  metrics=['accuracy'])
  return model
model = create_model()
model.summary()

checkpoint_path = "/content/drive/MyDrive/Colab Notebooks/System_ai/training_1/cp.ckpt"
checkpoint_dir = os.path.dirname(checkpoint_path)

# Create a callback that saves the model's weights
cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
                                                 save_weights_only=True,
                                                 verbose=1)

# Train the model with the new callback
model.fit(X_train, 
          y_train,
          epochs=20,
          validation_data=(X_test, y_test),
          callbacks=[cp_callback])

os.listdir(checkpoint_dir)

# Create a basic model instance
model = create_model()

# Evaluate the model
loss, acc = model.evaluate(X_test, y_test, verbose=2)
print("Untrained model, accuracy: {:5.2f}%".format(100 * acc))

# Loads the weights
model.load_weights(checkpoint_path)

# Re-evaluate the model
loss, acc = model.evaluate(X_test, y_test, verbose=2)
print("Restored model, accuracy: {:5.2f}%".format(100 * acc))

# Include the epoch in the file name (uses `str.format`)
checkpoint_path = "/content/drive/MyDrive/Colab Notebooks/System_ai/training_2/cp-{epoch:04d}.ckpt"
checkpoint_dir = os.path.dirname(checkpoint_path)

batch_size = 32

# Create a callback that saves the model's weights every 20 epochs
cp_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_path, 
    verbose=1, 
    save_weights_only=True,
    save_freq=5*batch_size)

# Create a new model instance
model = create_model()

# Save the weights using the `checkpoint_path` format
model.save_weights(checkpoint_path.format(epoch=35))

# Train the model with the new callback
model.fit(X_train, 
          y_train,
          epochs=20, 
          batch_size=batch_size, 
          callbacks=[cp_callback],
          validation_data=(X_test, y_test),
          verbose=0)

os.listdir(checkpoint_dir)

latest = tf.train.latest_checkpoint(checkpoint_dir)
latest

# Create a new model instance
model = create_model()

# Load the previously saved weights
model.load_weights(latest)

# Re-evaluate the model
loss, acc = model.evaluate(X_test, y_test, verbose=2)
print("Restored model, accuracy: {:5.2f}%".format(100 * acc))

# Save the weights
model.save_weights('/content/drive/MyDrive/Colab Notebooks/System_ai/checkpoints/my_checkpoint')

# Create a new model instance
model = create_model()

# Restore the weights
model.load_weights('/content/drive/MyDrive/Colab Notebooks/System_ai/checkpoints/my_checkpoint')

# Evaluate the model
loss, acc = model.evaluate(X_test, y_test, verbose=2)
print("Restored model, accuracy: {:5.2f}%".format(100 * acc))

# Create and train a new model instance.
model = create_model()
model.fit(X_train, y_train, epochs=35)

# Save the entire model as a SavedModel.
!mkdir -p saved_model
model.save('/content/drive/MyDrive/Colab Notebooks/System_ai/saved_model/my_model')

new_model = tf.keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/System_ai/saved_model/my_model')

# Check its architecture
new_model.summary()

# Create and train a new model instance.
model = create_model()
model.fit(X_train, y_train, epochs=20)

# Save the entire model to a HDF5 file.
# The '.h5' extension indicates that the model should be saved to HDF5.
model.save('/content/drive/MyDrive/Colab Notebooks/System_ai/my_model.h5')

# Recreate the exact same model, including its weights and the optimizer
new_model = tf.keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/System_ai/my_model.h5')

# Show the model architecture
new_model.summary()

loss, acc = new_model.evaluate(X_test, y_test, verbose=2)
print('Restored model, accuracy: {:5.2f}%'.format(100 * acc))

print(new_model.predict(X_test).shape)

from keras.models import load_model
from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
from keras.applications.vgg16 import preprocess_input
from keras.applications.vgg16 import decode_predictions
from keras.applications.vgg16 import VGG16
import numpy as np

image = load_img('/content/drive/MyDrive/Colab Notebooks/AI/img_dessert/go1.jpg', target_size=(100, 100))
img = np.array(image)
img = img / 255.0
img = img.reshape(1,100,100,3)
label = new_model.predict(img).argmax()
print("Predicted Class : ", label_thai[label])