# -*- coding: utf-8 -*-
"""NLTK_Ps_and_Neg_ercise.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/126V8ymy-2VYjV02DrWByiMxaNMmpDhJk
"""

import nltk                                # Python library for NLP
from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK
import matplotlib.pyplot as plt            # library for visualization
import random                              # pseudo-random number generator

# downloads sample twitter dataset. uncomment the line below if running on a local machine.
nltk.download('twitter_samples')

for name in twitter_samples.fileids():
    print(f" - {name}")

all_positive_tweets = twitter_samples.strings('positive_tweets.json')
all_negative_tweets = twitter_samples.strings('negative_tweets.json')

print('Number of positive tweets: ', len(all_positive_tweets))
print('Number of negative tweets: ', len(all_negative_tweets))

print('\nThe type of all_positive_tweets is: ', type(all_positive_tweets))
print('The type of a tweet entry is: ', type(all_negative_tweets[0]))
print(all_positive_tweets[5]);
print(all_negative_tweets[6]);

for tweet in all_positive_tweets:
    print(f" - {tweet}")

import pandas as pd
import numpy as np
df = pd.DataFrame(all_positive_tweets,columns=['Tweets'])
df

from wordcloud import WordCloud
allWords = ' '.join( [twts for twts in df['Tweets']])
wordCloud = WordCloud(width = 500, height=300, random_state = 21, max_font_size = 119).generate(allWords)

plt.imshow(wordCloud, interpolation = "bilinear")
plt.axis('off')
plt.show()

"""TextBlob เป็น Python Library สำหรับประมวลผลข้อมูลที่เป็น Text มันมี API ที่เรียบง่ายสำหรับการดำเนินการใน Natural Language Processing (NLP) Tasks เช่น Part-of-Speech Tagging, Noun Phrase Extraction, Sentiment Analysis, Classification, Translation และอื่น ๆ"""

from textblob import TextBlob
def getSubjectivity(text):
  return TextBlob(text).sentiment.subjectivity

def getPolarity(text):
  return TextBlob(text).sentiment.polarity

df['Subjectivity'] = df['Tweets'].apply(getSubjectivity) #ข้อความที่แสดงอัตวิสัย (ไม่ใช่ข้อเท็จจริง)

df['Polarity'] = df['Tweets'].apply(getPolarity)  #ระบุขั้วความรู้สึก  >0 คือ บวก

df

def getAnalysis(score):
    if score < 0 :
      return 'Negative'
    elif score == 0:
      return 'Neutral'
    else:
      return "Positive"

df["Analysis"] = df['Polarity'].apply(getAnalysis)

df

"""# Preprocess raw text for Sentiment analysis
Data preprocessing is one of the critical steps in any machine learning project. It includes cleaning and formatting the data before feeding into a machine learning algorithm. For NLP, the preprocessing steps are comprised of the following tasks:
 **ให้นักศึกษา preprocess ข้อความจาก twitter_samples ด้วยกระบวนการเหล่านี้**
1. Tokenizing the string
2. Regular expression
3. Removing stop words 
4. Text Lemmatization and Stemming
"""

import nltk
nltk.download('punkt') 
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')

"""# 1. Tokenizing the string"""

from nltk.tokenize import word_tokenize
tweetText = df['Tweets']
tweetText = tweetText.apply(word_tokenize)
tweetText

"""# 3. Removing stop words** 



"""

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
nltk.download('stopwords')

def my_stop_word(text):
  stop_words = set(stopwords.words('english'))

  filtered_sentence = [w for w in text if not w.lower() in stop_words]

  filtered_sentence = []

  for w in text:
      if w not in stop_words:
          filtered_sentence.append(w)
  print(filtered_sentence)

  return(filtered_sentence)

not_stop_word = [my_stop_word(txt) for txt in tweetText]

"""# 4. Text Lemmatization and Stemming"""

from nltk.stem import PorterStemmer 
from nltk.tokenize import word_tokenize 
   
ps = PorterStemmer()
def stp(text):
  ps_sent = [ps.stem(words_sent) for words_sent in text]

print(not_stop_word)
stem_word = [stp(txt) for txt in not_stop_word]
stem_word